---
title: "Decision_making_class_1"
output: html_document
date: '2023-09-18'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Building models


#### Install dependencies
```{r}
library(R2jags)
```

# Model 1: Fixed theta
```{r}
# rbinom(p, size, probability) # r=we are drawing randomly
# Three args: [p, size, prob] (p=number of trials/draws)


n_trials <- 100
Gfixed <- array(NA, c(n_trials)) # creates an empty array of length n_trials (100)
theta <- 0.7 # our skill level

for (t in 1:n_trials) { # for each trial...
    Gfixed[t] <- rbinom(1, 1, theta) # we are taking 1 sample at the time (summing to 100) at .7 probability due to skill level
}

sum(Gfixed)

plot(Gfixed)
```

# Fit model to simulation
```{r}
library(R2jags)

# The data for our model
data <- list("Gfixed", "n_trials")

# The parameter we want to estimate
params <- "theta"
# given that the chicken sexer gets 73 correct guesses out of 100, what is his skill level?

fixed_samples <- jags(data, inits=NULL, params, model.file="Module 1/chick_jags.txt", n.chains=3, n.iter=5000, n.burnin=1000, n.thin=1)

# Now we've estimated theta (his skill level) given how the chickensexing went over 100 trials

plot(density(fixed_samples$BUGSoutput$sims.list$theta[,1]))
```
**Variable explanation**
n.chains = kindda like epochs: the number of times we want to run the model
n.iter= the number of samples to be drawn in order to get an estimate of the underlying parameter
n.burnin = warmup: ignoring the first number of burnings
n.thin = 1 means that we don't thin, should mostly be at 1

# Model 2: Learning models
```{r}
n_trials <- 100
Glearn <- array(NA, c(n_trials)) # empty array which ends up containing the chickensexers guesses for each trial

theta_learn <- array(NA, c(n_trials)) # our skill level (which is now subject to change (starting at 0.00001)))
alpha <- 0.07 # learning rate
theta_1 <- 0.0001 # our initial skill level

theta_learn[1] <- theta_1 # our initial skill level (we hardcode this into the first position in the theta_learn array)
Glearn[1] <- rbinom(1,1,theta_1) # out first guess based on our initial skill level,hardcoded into the chickensexer guess array

for (t in 2:n_trials){ # we are starting from 2, because 1 is our initial guess based on the initial level (0.5)
    theta_learn[t] <- theta_learn[t - 1]^(1 / (1 + alpha)) #here we are updating our skill level based on the previous skill level and the learning rate
                                # we are updating the skill level for each t based on the previous t to the power of 1/(1+alpha) (the learning rate).
    Glearn[t] <- rbinom(1, 1, theta_learn[t]) # here we are updating our guess based on our updated skill level, drawn from a binomial distribution
    print(paste0("Trial ", t, ": theta_learn = ", theta_learn[t], ", Glearn = ", Glearn[t])) # printing each trial run

}

#plot(Glearn)
plot(theta_learn)
plot(Glearn)
```

# fit learning rate similuations to learning model
```{r}
# The data for our model
data <- list("Glearn" = Glearn, "n_trials" = n_trials)
# The parameters we want to estimate
params <- c("theta_1", "alpha", "theta_learn") 

learn_samples <- jags(data, inits=NULL, params, model.file="Module 1/chick_learn_jags.txt", n.chains=3, n.iter=5000, n.burnin=1000, n.thin=1)

#fixed_samples$BUGSoutput$sims.list$theta

plot(density(learn_samples$BUGSoutput$sims.list$theta_1))
plot(density(learn_samples$BUGSoutput$sims.list$alpha))
```

# model 3: Learning with multiple n_agents
```{r}
nseeds = 10 # number of "experiments" to runs

n_trials <- 100
n_agents <- 100
Glearn <- array(NA, c(n_trials)) # our number of guesses array (empty as of now)

theta_learn <- array(NA, c(n_trials)) # our skill level (which is now subject to change (starting at 0.5)))
alpha <- 0.01 # learning rate
theta_1 <- 0.5 # our initial skill level

theta_learn[,1] <- theta_1 # our initial skill level
Glearn [ ,1] <- rbinom(1,1,theta_1)


for (s in 1:n_agents){
    for (t in 2:n_trials) { # we are starting from 2, because 1 is our initial level (0.5)
        theta_learn[s,t] <- theta_learn[s, t - 1]^(1 / (1 + alpha)) # 
        Glearn[s,t] <- rbinom(1, 1, theta_learn[s,t])
        #Glearn[1] <- rbinom(1, 1, theta_1[s,t])
    }
}

#plot(Glearn)
mean(colSums(Glearn))
```




```{r}
# Model -- w. fatique and increased concentration (memory boost) after wrong guess
n_trials <- 500
G_learn <- array(NA, c(n_trials)) # success = 1, fail = 0
skill_level <- array(NA, c(n_trials))
theta_1 <- 0.5
alpha <- 0.01

fatigue <- array(NA, c(n_trials))
guess_prob <- array(NA, c(n_trials))

# Memory Effect parameters
memory_boost <- 0.05 # how much to boost theta_learn after a wrong guess
boost_duration <- 8 # how many trials the boost lasts for
trials_since_mistake <- Inf # track how many trials since last mistake

# First trial
skill_level[1] <- theta_1
fatigue[1] <- 0.01
guess_prob[1] <- skill_level[1]*(1-fatigue[1]) # will practically be .5
guess_prob[1] <- max(0, min(1, guess_prob[1])) # between 1 and 0
G_learn[1] <- rbinom(1,1,guess_prob[1])

# If a mistake is made on the first trial
if(G_learn[1] == 0) trials_since_mistake <- 1

for(t in 2:n_trials){
  skill_level[t] <- skill_level[t-1]^(1/(1+alpha))
  fatigue[t] <- 0 + (0.4/(1+exp(-0.04*(t-250)))^(1/1))
  
  guess_prob[t] <- skill_level[t]*(1-fatigue[t])
  
  # apply the memory effect if within the boost_duration since last mistake
  if(trials_since_mistake <= boost_duration){
    guess_prob[t] <- guess_prob[t] + memory_boost
  }

  # Ensure that guess prob between 0 and 1
  guess_prob[t] <- max(0, min(1, guess_prob[t]))
  
  G_learn[t] <- rbinom(1,1,guess_prob[t])
  
  # if a mistake is made, reset trials_since_mistake
  if(!is.na(G_learn[t]) && G_learn[t] == 0){
    
    trials_since_mistake <- 1
  } else {
    trials_since_mistake <- trials_since_mistake + 1
  }
}

# Plotting
colors <- ifelse(G_learn == 1, "red", "blue")
plot(skill_level, col = "forestgreen")+title("Skill level")
plot(guess_prob, col = colors)+title("Success = red, Fail = blue")

# plot(G_learn)
```