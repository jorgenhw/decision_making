---
title: "Decision_making_class_1"
output: html_document
date: '2023-09-18'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Building models


#### Install dependencies
```{r}
library(R2jags)
```

# Model 1: Fixed theta
```{r}
# rbinom(p, size, probability) # r=we are drawing randomly
# Three args: [p, size, prob] (p=number of trials/draws)


n_trials <- 100
Gfixed <- array(NA, c(n_trials)) # our number of guesses array (empty as of now)
theta <- 0.7 # our skill level

for (t in 1:n_trials) {
    Gfixed[t] <- rbinom(1, 1, theta) # we are taking 1 sample at the time at .7 probability due to skill level
}

sum(Gfixed)

plot(Gfixed)
```

# Fit model to simulation
```{r}
library(R2jags)

# The data for our model
data <- list("Gfixed" = Gfixed, "n_trials" = n_trials)
# The parameters we want to estimate
params <- c("theta")

fixed_samples <- jags(data, inits=NULL, params, model.file="Module 1/chick_jags.txt", n.chains=3, n.iter=5000, n.burnin=1000, n.thin=1)

plot(density(fixed_samples$BUGSoutput$sims.list$theta[,1]))
```

```{r}

```


n.chains = kindda like epochs: the number of times we want to run the model
n.iter= the number of samples to be drawn in order to get an estimate of the underlying parameter
n.burnin = warmup: ignoring the first number of burnings
n.thin = 1 means that we don't thin, should mostly be at 1

# Model 2: Learning models
```{r}
n_trials <- 100
Glearn <- array(NA, c(n_trials)) # our number of guesses array (empty as of now)

theta_learn <- array(NA, c(n_trials)) # our skill level (which is now subject to change (starting at 0.5)))
alpha <- 0.01 # learning rate
theta_1 <- 0.5 # our initial skill level

theta_learn[1] <- theta_1 # our initial skill level
Glearn [1] <_ rbinom(1,1,theta1)


for (t in 2:n_trials){ # we are starting from 2, because 1 is our initial level (0.5)
    theta_learn[t] <- theta_learn[t - 1]^(1 / (1 + alpha)) # 
    Glearn[t] <- rbinom(1, 1, theta_learn[t])
    Glearn[1] <- rbinom(1, 1, theta_1[t])
}

#plot(Glearn)
plot(theta_learn)
```

# fit learning rate similuations to learning model
```{r}
# The data for our model
data <- list("Glearn" = Glearn, "n_trials" = n_trials)
# The parameters we want to estimate
params <- c("theta1", "alpha", "theta_learn") 

learn_samples <- jags(data, inits=NULL, params, model.file="Module 1/chick_learn_jags.txt", n.chains=3, n.iter=5000, n.burnin=1000, n.thin=1)

#fixed_samples$BUGSoutput$sims.list$theta

plot(density(learn_samples$BUGSoutput$sims.list$theta1))
plot(density(learn_samples$BUGSoutput$sims.list$alpha))
```

# model 3: Learning with multiple n_agents
```{r}
nseeds = 10 # number of "experiments" to run


n_trials <- 100
n_agents <- 100
Glearn <- array(NA, c(n_trials)) # our number of guesses array (empty as of now)

theta_learn <- array(NA, c(n_trials)) # our skill level (which is now subject to change (starting at 0.5)))
alpha <- 0.01 # learning rate
theta_1 <- 0.5 # our initial skill level

theta_learn[,1] <- theta_1 # our initial skill level
Glearn [ ,1] <- rbinom(1,1,theta_1)


for (s in 1:n_agents){
    for (t in 2:n_trials) { # we are starting from 2, because 1 is our initial level (0.5)
        theta_learn[s,t] <- theta_learn[s, t - 1]^(1 / (1 + alpha)) # 
        Glearn[s,t] <- rbinom(1, 1, theta_learn[s,t])
        #Glearn[1] <- rbinom(1, 1, theta_1[s,t])
    }
}

#plot(Glearn)
mean(colSums(Glearn))
```